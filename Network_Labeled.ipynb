{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fffd31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43e6d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logger = logging.getLogger()\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "settings = dict()\n",
    "store = dict()\n",
    "\n",
    "settings = {\n",
    "    \"data_root\": \"handwriting_cvl_data/words/\",\n",
    "    \"manual_seed\": 42,\n",
    "    \"nc\": 1,\n",
    "    \"nz\": 100,\n",
    "    \"ngf\": 64,\n",
    "    \"ndf\": 64,\n",
    "    \"num_workers\": 32,\n",
    "    \"batch_size\": 128,\n",
    "    \"num_epochs\": 5,\n",
    "    \"lr\": 0.0002,\n",
    "    \"beta1\": 0.5,\n",
    "    \"image_size\": (64, 64),\n",
    "    \"ncond\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f44b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithClasses(ImageFolder):\n",
    "    def __getitem__(self, idx):\n",
    "        item = super().__getitem__(idx)\n",
    "        paths = self.imgs[idx][0].split(\"/\")\n",
    "        filename = paths[len(paths) - 1].split(\".\")[0]\n",
    "        writer_id, page_num, line_num, word_num, word = filename.split(\"-\", 4)\n",
    "        return (item + (word, writer_id,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfdb686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(initial=False):\n",
    "    if initial:\n",
    "        root_logger.setLevel(logging.INFO)\n",
    "        root_logger.addHandler(logging.FileHandler(\"Network_Labeled.log\", \"w\", \"utf-8\"))\n",
    "\n",
    "    if \"manual_seed\" in settings:\n",
    "        manual_seed = settings[\"manual_seed\"]\n",
    "        random.seed(manual_seed)\n",
    "        torch.manual_seed(manual_seed)\n",
    "        print(f\"[setup] Set manual seed to: {manual_seed}\")\n",
    "\n",
    "setup(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    data_root = settings[\"data_root\"]\n",
    "    batch_size = settings[\"batch_size\"]\n",
    "    image_size = settings[\"image_size\"]\n",
    "    num_workers = settings[\"num_workers\"]\n",
    "\n",
    "    dataset = ImageFolderWithClasses(root=data_root, transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5)),\n",
    "    ]))\n",
    "\n",
    "    words = set()\n",
    "    writers = set()\n",
    "    for i in range(len(dataset)):\n",
    "        words.add(dataset[i][2])\n",
    "        writers.add(dataset[i][3])\n",
    "\n",
    "    words = sorted(list(words))\n",
    "    word_to_idx = { word: i for i, word in enumerate(words) }\n",
    "\n",
    "    writers = sorted(list(writers))\n",
    "    writer_to_idx = { writer: i for i, writer in enumerate(writers) }\n",
    "\n",
    "    store[\"dataloader\"] = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "    store[\"dataset\"] = dataset\n",
    "\n",
    "    store[\"nwords\"] = len(words)\n",
    "    store[\"nwriters\"] = len(writers)\n",
    "\n",
    "    store[\"word_to_idx\"] = word_to_idx\n",
    "    store[\"writer_to_idx\"] = writer_to_idx\n",
    "    store[\"idx_to_word\"] = { val: key for key, val in store[\"word_to_idx\"].items() }\n",
    "    store[\"idx_to_class\"] = { val: key for key, val in store[\"writer_to_idx\"].items() }\n",
    "    store[\"device\"] = torch.device(\"cpu\")\n",
    "\n",
    "setup()\n",
    "load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faef9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    m_type = type(m)\n",
    "    if m_type in [nn.ConvTranspose2d, nn.Conv2d]:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif m_type in [nn.BatchNorm2d]:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba227fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = settings[\"nc\"]\n",
    "nz = settings[\"nz\"]\n",
    "ngf = settings[\"ngf\"]\n",
    "ncond = settings[\"ncond\"]\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.writer_embedding = nn.Embedding(store[\"nwriters\"], nz)\n",
    "        self.writer_label = nn.Sequential(\n",
    "            self.writer_embedding,\n",
    "            nn.Linear(nz, 1)\n",
    "        )\n",
    "\n",
    "        self.word_embedding = nn.Embedding(store[\"nwords\"], nz)\n",
    "        self.word_label = nn.Sequential(\n",
    "            self.word_embedding,\n",
    "            nn.Linear(nz, 1)\n",
    "        )\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz + ncond, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, writer_label_input, word_label_input, image_input):\n",
    "        generated_writer_label = self.writer_label(writer_label_input).view(-1, 1, 1, 1)\n",
    "        generated_word_label = self.word_label(word_label_input).view(-1, 1, 1, 1)\n",
    "        return self.main(torch.cat((image_input, generated_writer_label, generated_word_label), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_generator():\n",
    "    store[\"netG\"] = Generator().to(store[\"device\"])\n",
    "    store[\"netG\"].apply(init_weights)\n",
    "    print(store[\"netG\"])\n",
    "\n",
    "setup()\n",
    "construct_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97e67315",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = settings[\"nc\"]\n",
    "nz = settings[\"nz\"]\n",
    "ndf = settings[\"ndf\"]\n",
    "ncond = settings[\"ncond\"]\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.writer_embedding = nn.Embedding(store[\"nwriters\"], nz)\n",
    "        self.word_embedding = nn.Embedding(store[\"nwords\"], nz)\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc + ncond * nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def label(self, embedding, image_size):\n",
    "        return nn.Sequential(\n",
    "            embedding,\n",
    "            nn.Linear(nz, ceil(nc * image_size * image_size))\n",
    "        )\n",
    "\n",
    "    def forward(self, writer_label_input, word_label_input, image_input):\n",
    "        image_size = image_input.shape[3]\n",
    "        discriminated_writer_label = self.label(self.writer_embedding, image_size)(writer_label_input)\n",
    "        discriminated_word_label = self.label(self.word_embedding, image_size)(word_label_input)\n",
    "        return self.main(torch.cat((\n",
    "            image_input,\n",
    "            discriminated_writer_label.view(-1, nc, image_size, image_size),\n",
    "            discriminated_word_label.view(-1, nc, image_size, image_size),\n",
    "        ), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df798b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_discriminator():\n",
    "    store[\"netD\"] = Discriminator().to(store[\"device\"])\n",
    "    store[\"netD\"].apply(init_weights)\n",
    "    print(store[\"netD\"])\n",
    "\n",
    "setup()\n",
    "construct_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    dataloader = store[\"dataloader\"]\n",
    "    device = store[\"device\"]\n",
    "    netD = store[\"netD\"]\n",
    "    netG = store[\"netG\"]\n",
    "\n",
    "    num_epochs = settings[\"num_epochs\"]\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=settings[\"lr\"], betas=(settings[\"beta1\"], 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=settings[\"lr\"], betas=(settings[\"beta1\"], 0.999))\n",
    "    criterion = nn.BCELoss()\n",
    "    real_label = 1.0\n",
    "    fake_label = 0.0\n",
    "\n",
    "    def train_discriminator(data):\n",
    "        netD.zero_grad()\n",
    "        real_data = data[0].to(device)\n",
    "        b_size = real_data.size(0)\n",
    "        real_writers = torch.tensor([store[\"writer_to_idx\"][writer] for writer in data[3]]).unsqueeze(1)\n",
    "        real_words = torch.tensor([store[\"word_to_idx\"][word] for word in data[2]]).unsqueeze(1)\n",
    "\n",
    "        # discriminate real image data (stacked with the real conditional labels)\n",
    "        real_output = netD(real_writers, real_words, real_data)\n",
    "        real_output = real_output.view(-1)\n",
    "\n",
    "        # calculate error using tensor of real_labels\n",
    "        # the goal is to get discriminator to detect real instances\n",
    "        real_labels = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        errD_real = criterion(real_output, real_labels)\n",
    "\n",
    "        # calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "\n",
    "        # generate fake data (from real conditional labels and noise)\n",
    "        # and detach gradients from the output because we're training the discriminator\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        generated = netG(real_writers, real_words, noise).detach()\n",
    "\n",
    "        # discriminate fake image data (stacked with the real conditional labels, which the data was generated from)\n",
    "        fake_output = netD(real_writers, real_words, generated).view(-1)\n",
    "\n",
    "        # calculate error using tensor of fake_labels\n",
    "        # the goal is to get discriminator to detect fake instances\n",
    "        fake_labels = torch.full((b_size,), fake_label, dtype=torch.float, device=device)\n",
    "        errD_fake = criterion(fake_output, fake_labels)\n",
    "\n",
    "        # calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "\n",
    "        # return loss and discriminator mean \n",
    "        optimizerD.step()\n",
    "        return errD_real + errD_fake\n",
    "\n",
    "    def train_generator(data):\n",
    "        netG.zero_grad()\n",
    "        real_data = data[0].to(device)\n",
    "        b_size = real_data.size(0)\n",
    "        rand_writers = torch.randint(0, store[\"nwriters\"], (b_size, 1), device=device)\n",
    "        rand_words = torch.randint(0, store[\"nwords\"], (b_size, 1), device=device)\n",
    "\n",
    "        # generate fake data (from random conditional labels and noise)\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        generated_data = netG(rand_writers, rand_words, noise)\n",
    "\n",
    "        # discriminate fake image data (stacked with the random conditional labels, which the data was generated from)\n",
    "        fake_output = netD(rand_writers, rand_words, generated_data).view(-1)\n",
    "\n",
    "        # calculate error using tensor of real_labels\n",
    "        # the goal is to get the generator to generate \"real\" instances\n",
    "        real_labels = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        errG = criterion(fake_output, real_labels)\n",
    "\n",
    "        # calculate gradients for G\n",
    "        errG.backward()\n",
    "\n",
    "        # return loss and discriminator mean \n",
    "        optimizerG.step()\n",
    "        return errG\n",
    "\n",
    "    root_logger.info(\"Starting training loop...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            errD = train_discriminator(data)\n",
    "            errG = train_generator(data)\n",
    "\n",
    "            D_losses.append(errD.item())\n",
    "            G_losses.append(errG.item())\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                epoch_progress = f\"{epoch}/{num_epochs} epochs\"\n",
    "                iteration_progress = f\"{i}/{len(dataloader)} iterations\"\n",
    "                root_logger.info(f\"{epoch_progress}, {iteration_progress}, Loss_D: {errD.item()}, Loss_G: {errG.item()}\")\n",
    "\n",
    "    torch.save(netG.state_dict(), \"models/Network_Labeled.G.pth\")\n",
    "    torch.save(netD.state_dict(), \"models/Network_Labeled.D.pth\")\n",
    "\n",
    "    root_logger.info(\"training complete\")\n",
    "    root_logger.info(\"models saved\")\n",
    "\n",
    "setup()\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    dataloader = store[\"dataloader\"]\n",
    "    device = store[\"device\"]\n",
    "    netG = store[\"netG\"]\n",
    "    nz = settings[\"nz\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Iterations vs. Loss\")\n",
    "    plt.plot(G_losses, label=\"G\")\n",
    "    plt.plot(D_losses, label=\"D\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    real_images = next(iter(dataloader))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(np.transpose(vutils.make_grid(real_images[0].to(device)[:64], padding=5, normalize=True).cpu(),(1, 2, 0)))\n",
    "\n",
    "    rand_writers = torch.randint(0, store[\"nwriters\"], (64, 1), device=device)\n",
    "    rand_words = torch.randint(0, store[\"nwords\"], (64, 1), device=device)\n",
    "    noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "    fake_images = netG(rand_writers, rand_words, noise).detach().cpu()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(np.transpose(vutils.make_grid(fake_images, padding=2, normalize=True).cpu(), (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
